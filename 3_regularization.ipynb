{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_lamda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels], stddev=0.1))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + tf_lamda*tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding the value of lamda is non-trivial. We should try multiple values to find out one which works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Test accuracy: 89.3%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.4%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.3%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.3%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.1%\n",
      "Initialized\n",
      "Test accuracy: 89.2%\n",
      "Initialized\n",
      "Test accuracy: 89.1%\n",
      "Initialized\n",
      "Test accuracy: 89.1%\n",
      "Initialized\n",
      "Test accuracy: 89.1%\n",
      "Initialized\n",
      "Test accuracy: 89.1%\n",
      "Initialized\n",
      "Test accuracy: 89.0%\n",
      "Initialized\n",
      "Test accuracy: 89.0%\n",
      "Initialized\n",
      "Test accuracy: 88.9%\n",
      "Initialized\n",
      "Test accuracy: 88.7%\n",
      "Initialized\n",
      "Test accuracy: 88.6%\n",
      "Initialized\n",
      "Test accuracy: 88.4%\n",
      "Initialized\n",
      "Test accuracy: 88.3%\n",
      "Initialized\n",
      "Test accuracy: 88.1%\n",
      "Initialized\n",
      "Test accuracy: 87.8%\n",
      "Initialized\n",
      "Test accuracy: 87.6%\n",
      "Initialized\n",
      "Test accuracy: 87.2%\n",
      "Initialized\n",
      "Test accuracy: 86.9%\n",
      "Initialized\n",
      "Test accuracy: 86.5%\n",
      "Initialized\n",
      "Test accuracy: 85.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niXEKAhGRGKcgEBGJcf8f7LP3GXkxkbsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x142c70fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "lamdas = [pow(10, i) for i in np.arange(-4, -1, 0.1)]\n",
    "#lamda = 0.0000001\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for lamda in lamdas:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_lamda : lamda}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#         if (step % 500 == 0):\n",
    "#           print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#           print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#           print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#             valid_prediction.eval(), valid_labels))\n",
    "      accuracies.append(accuracy(test_prediction.eval(), test_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "plt.semilogx(lamdas, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.38\n",
      "lamda =  0.000158489319246\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = np.max(accuracies)\n",
    "i = accuracies.index(max_accuracy)\n",
    "print(max_accuracy)\n",
    "print ('lamda = ', lamdas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test accuracy for L2 regularization of logisitic regression for the same number of iterations gives us a slightly better test accuracy. Below, we try regularization for Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_lamda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size], stddev=0.1))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels], stddev=0.1))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  relu_output = tf.nn.relu(layer1)\n",
    "  logits = tf.matmul(relu_output, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "               + tf_lamda*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2)+biases2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Test accuracy: 94.2%\n",
      "Initialized\n",
      "Test accuracy: 94.4%\n",
      "Initialized\n",
      "Test accuracy: 94.2%\n",
      "Initialized\n",
      "Test accuracy: 94.1%\n",
      "Initialized\n",
      "Test accuracy: 94.3%\n",
      "Initialized\n",
      "Test accuracy: 94.2%\n",
      "Initialized\n",
      "Test accuracy: 94.2%\n",
      "Initialized\n",
      "Test accuracy: 94.4%\n",
      "Initialized\n",
      "Test accuracy: 94.2%\n",
      "Initialized\n",
      "Test accuracy: 94.1%\n",
      "Initialized\n",
      "Test accuracy: 93.9%\n",
      "Initialized\n",
      "Test accuracy: 93.8%\n",
      "Initialized\n",
      "Test accuracy: 93.4%\n",
      "Initialized\n",
      "Test accuracy: 93.1%\n",
      "Initialized\n",
      "Test accuracy: 92.8%\n",
      "Initialized\n",
      "Test accuracy: 92.5%\n",
      "Initialized\n",
      "Test accuracy: 92.1%\n",
      "Initialized\n",
      "Test accuracy: 91.7%\n",
      "Initialized\n",
      "Test accuracy: 91.3%\n",
      "Initialized\n",
      "Test accuracy: 90.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104541150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "lamdas = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for lamda in lamdas:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_lamda: lamda}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#         if (step % 500 == 0):\n",
    "#           print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#           print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#           print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#             valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      accuracies.append(accuracy(test_prediction.eval(), test_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "plt.semilogx(lamdas, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.38\n",
      "lamda =  0.000501187233627\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = np.max(accuracies)\n",
    "i = accuracies.index(max_accuracy)\n",
    "print(max_accuracy)\n",
    "print ('lamda = ', lamdas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test accuracy for L2 regularization of neural network for the same number of iterations gives us a slightly better test accuracy. Below, we try regularization for Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_lamda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  relu_output = tf.nn.relu(layer1)\n",
    "  logits = tf.matmul(relu_output, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2)+biases2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 317.402435\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 38.0%\n",
      "Test accuracy: 79.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "lamda = 0\n",
    "num_batches = 2\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "   \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_lamda: lamda}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "\n",
    "  accuracies.append(accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_lamda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  relu_output = tf.nn.relu(layer1)\n",
    "  dropout_output = tf.nn.dropout(relu_output, 0.5)\n",
    "  logits = tf.matmul(dropout_output, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "               + tf_lamda*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2)+biases2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 534.541626\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 24.8%\n",
      "Minibatch loss at step 10: 55.937737\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 20: 10.855789\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 30: 8.063570\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 40: 4.904087\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 50: 9.732141\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 60: 1.779456\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 70: 1.742350\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 80: 0.240998\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 90: 0.494480\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 100: 0.582684\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 81.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "lamda = 0\n",
    "num_batches = 2\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "   \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_lamda: lamda}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "\n",
    "  accuracies.append(accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "hidden_size2 = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_lamda = tf.placeholder(tf.float32)\n",
    "\n",
    "  \n",
    "  # Variables.\n",
    "  global_step = tf.Variable(0)\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size], stddev=0.01))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, hidden_size2], stddev=0.1))\n",
    "  biases2 = tf.Variable(tf.zeros([hidden_size2]))\n",
    "\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size2, num_labels], stddev=0.1))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  layer1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  relu_output = tf.nn.relu(layer1)\n",
    "  dropout_output = tf.nn.dropout(relu_output, 0.5)\n",
    "\n",
    "  layer2 = tf.matmul(dropout_output, weights2) + biases2\n",
    "  relu_output2 = tf.nn.relu(layer2)\n",
    "  dropout_output2 = tf.nn.dropout(relu_output2, 0.5)\n",
    "    \n",
    "  logits = tf.matmul(dropout_output2, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "               + tf_lamda*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  lr = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(\n",
    "    tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2)+biases2), weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(\n",
    "    tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2)+biases2), weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.577324\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 46.1%\n",
      "Minibatch loss at step 500: 0.602426\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1000: 0.705745\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 1500: 0.563996\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2000: 0.504436\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2500: 0.587597\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 0.684582\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3500: 0.683689\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4000: 0.611462\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 4500: 0.523897\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.565277\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.593734\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6000: 0.661891\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 6500: 0.413756\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 7000: 0.623401\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 7500: 0.554449\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 8000: 0.720665\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 0.481932\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9000: 0.541076\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 9500: 0.517814\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10000: 0.462571\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 10500: 0.454491\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 11000: 0.421932\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 11500: 0.465599\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 12000: 0.641042\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12500: 0.473995\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 13000: 0.497844\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 13500: 0.468021\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 14000: 0.445317\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14500: 0.555693\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15000: 0.464200\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 95.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "lamda = 0.000501187233627\n",
    "lamda = 0.000158489319246\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "   \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_lamda: lamda}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "\n",
    "  accuracies.append(accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
